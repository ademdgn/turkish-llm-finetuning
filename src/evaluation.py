"""
Model Evaluation Module
EÄŸitilmiÅŸ modelin kapsamlÄ± deÄŸerlendirmesi
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_curve, 
    auc,
    precision_recall_curve
)
from datasets import load_from_disk
import warnings
warnings.filterwarnings('ignore')

class ModelEvaluator:
    """
    Model deÄŸerlendirme sÄ±nÄ±fÄ±
    """
    
    def __init__(self, model_path="./models/final", dataset_path="data/processed/tokenized_dataset"):
        """
        Args:
            model_path (str): EÄŸitilmiÅŸ model yolu
            dataset_path (str): Test dataset yolu
        """
        self.model_path = model_path
        self.dataset_path = dataset_path
        self.label_names = ["NEGATIVE", "NEUTRAL", "POSITIVE"]
        
        # Model ve tokenizer yÃ¼kle
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
            self.classifier = pipeline(
                "text-classification",
                model=self.model,
                tokenizer=self.tokenizer,
                return_all_scores=True
            )
            print(f"âœ… Model yÃ¼klendi: {model_path}")
        except Exception as e:
            print(f"âŒ Model yÃ¼klenemedi: {str(e)}")
            self.model = None
            return
        
        # Dataset yÃ¼kle
        try:
            self.dataset = load_from_disk(dataset_path)
            print(f"âœ… Dataset yÃ¼klendi: {dataset_path}")
        except Exception as e:
            print(f"âŒ Dataset yÃ¼klenemedi: {str(e)}")
            self.dataset = None
    
    def evaluate_model(self):
        """
        Modeli kapsamlÄ± ÅŸekilde deÄŸerlendir
        
        Returns:
            dict: DeÄŸerlendirme sonuÃ§larÄ±
        """
        if self.model is None or self.dataset is None:
            print("âŒ Model veya dataset yÃ¼klenmemiÅŸ!")
            return None
        
        print("ğŸ“Š Model deÄŸerlendirmesi baÅŸlÄ±yor...")
        
        # Test verilerini hazÄ±rla
        test_dataset = self.dataset['test']
        
        # Tahminleri al
        print("ğŸ”® Tahminler hesaplanÄ±yor...")
        predictions, probabilities, true_labels = self._get_predictions(test_dataset)
        
        # Temel metrikler
        metrics = self._calculate_metrics(true_labels, predictions, probabilities)
        
        # GÃ¶rselleÅŸtirmeler
        self._create_visualizations(true_labels, predictions, probabilities)
        
        # Hata analizi
        error_analysis = self._analyze_errors(test_dataset, true_labels, predictions)
        
        # SonuÃ§larÄ± kaydet
        results = {
            "metrics": metrics,
            "error_analysis": error_analysis,
            "model_path": self.model_path,
            "dataset_path": self.dataset_path
        }
        
        self._save_results(results)
        
        return results
    
    def _get_predictions(self, test_dataset):
        """
        Test verisi iÃ§in tahminleri al
        """
        predictions = []
        probabilities = []
        true_labels = []
        
        # Model'i evaluation moduna al
        self.model.eval()
        device = next(self.model.parameters()).device
        
        with torch.no_grad():
            for i, example in enumerate(test_dataset):
                if i % 100 == 0:
                    print(f"Ä°ÅŸlenen: {i}/{len(test_dataset)}")
                
                # Tahmin yap - tensor'larÄ± GPU'ya taÅŸÄ±
                inputs = {
                    'input_ids': torch.tensor([example['input_ids']]).to(device),
                    'attention_mask': torch.tensor([example['attention_mask']]).to(device)
                }
                
                outputs = self.model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
                
                pred = torch.argmax(probs, dim=-1).item()
                prob = probs[0].cpu().numpy()
                
                predictions.append(pred)
                probabilities.append(prob)
                true_labels.append(example['labels'])  # 'label' -> 'labels'
        
        return np.array(predictions), np.array(probabilities), np.array(true_labels)
    
    def _calculate_metrics(self, true_labels, predictions, probabilities):
        """
        DeÄŸerlendirme metriklerini hesapla
        """
        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
        
        metrics = {
            "accuracy": accuracy_score(true_labels, predictions),
            "f1_macro": f1_score(true_labels, predictions, average='macro'),
            "f1_weighted": f1_score(true_labels, predictions, average='weighted'),
            "precision_macro": precision_score(true_labels, predictions, average='macro'),
            "recall_macro": recall_score(true_labels, predictions, average='macro')
        }
        
        # SÄ±nÄ±f bazÄ±nda metrikler
        class_report = classification_report(
            true_labels, predictions, 
            target_names=self.label_names, 
            output_dict=True
        )
        
        metrics["classification_report"] = class_report
        
        print("ğŸ“ˆ DeÄŸerlendirme sonuÃ§larÄ±:")
        print(f"   - Accuracy: {metrics['accuracy']:.4f}")
        print(f"   - F1 (macro): {metrics['f1_macro']:.4f}")
        print(f"   - F1 (weighted): {metrics['f1_weighted']:.4f}")
        print(f"   - Precision (macro): {metrics['precision_macro']:.4f}")
        print(f"   - Recall (macro): {metrics['recall_macro']:.4f}")
        
        return metrics
    
    def _create_visualizations(self, true_labels, predictions, probabilities):
        """
        GÃ¶rselleÅŸtirmeler oluÅŸtur
        """
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        # 1. Confusion Matrix
        plt.figure(figsize=(10, 8))
        cm = confusion_matrix(true_labels, predictions)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.label_names,
                   yticklabels=self.label_names)
        plt.title('Confusion Matrix')
        plt.ylabel('GerÃ§ek Label')
        plt.xlabel('Tahmin Edilen Label')
        plt.tight_layout()
        plt.savefig(results_dir / "confusion_matrix_detailed.png", dpi=300, bbox_inches='tight')
        plt.show()
        
        # 2. SÄ±nÄ±f bazÄ±nda doÄŸruluk daÄŸÄ±lÄ±mÄ±
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        for i, label_name in enumerate(self.label_names):
            class_probs = probabilities[true_labels == i, i]
            axes[i].hist(class_probs, bins=20, alpha=0.7, color=f'C{i}')
            axes[i].set_title(f'{label_name} SÄ±nÄ±fÄ± GÃ¼ven SkorlarÄ±')
            axes[i].set_xlabel('GÃ¼ven Skoru')
            axes[i].set_ylabel('Frekans')
        
        plt.tight_layout()
        plt.savefig(results_dir / "confidence_distributions.png", dpi=300, bbox_inches='tight')
        plt.show()
        
        # 3. Hata daÄŸÄ±lÄ±mÄ±
        incorrect_predictions = true_labels != predictions
        error_confidence = np.max(probabilities[incorrect_predictions], axis=1)
        correct_confidence = np.max(probabilities[~incorrect_predictions], axis=1)
        
        plt.figure(figsize=(10, 6))
        plt.hist(correct_confidence, bins=20, alpha=0.7, label='DoÄŸru Tahminler', color='green')
        plt.hist(error_confidence, bins=20, alpha=0.7, label='YanlÄ±ÅŸ Tahminler', color='red')
        plt.xlabel('Maksimum GÃ¼ven Skoru')
        plt.ylabel('Frekans')
        plt.title('DoÄŸru vs YanlÄ±ÅŸ Tahminlerin GÃ¼ven SkorlarÄ±')
        plt.legend()
        plt.tight_layout()
        plt.savefig(results_dir / "error_confidence_analysis.png", dpi=300, bbox_inches='tight')
        plt.show()
    
    def _analyze_errors(self, test_dataset, true_labels, predictions):
        """
        Hata analizi yap
        """
        # YanlÄ±ÅŸ tahminleri bul
        incorrect_mask = true_labels != predictions
        incorrect_indices = np.where(incorrect_mask)[0]
        
        # Hata tÃ¼rlerini analiz et
        error_types = {}
        for true_label, pred_label in zip(true_labels[incorrect_mask], predictions[incorrect_mask]):
            error_key = f"{self.label_names[true_label]} -> {self.label_names[pred_label]}"
            error_types[error_key] = error_types.get(error_key, 0) + 1
        
        print("ğŸ” Hata analizi:")
        print(f"   - Toplam hata: {len(incorrect_indices)}")
        print(f"   - Hata oranÄ±: {len(incorrect_indices)/len(true_labels):.4f}")
        print("   - Hata tÃ¼rleri:")
        for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
            print(f"     {error_type}: {count}")
        
        # En Ã§ok hata yapÄ±lan Ã¶rnekleri kaydet
        error_examples = []
        for i in incorrect_indices[:10]:  # Ä°lk 10 hata
            example = test_dataset[int(i)]
            error_examples.append({
                "text": self.tokenizer.decode(example['input_ids'], skip_special_tokens=True),
                "true_label": self.label_names[true_labels[i]],
                "predicted_label": self.label_names[predictions[i]],
                "index": int(i)
            })
        
        return {
            "total_errors": len(incorrect_indices),
            "error_rate": len(incorrect_indices)/len(true_labels),
            "error_types": error_types,
            "error_examples": error_examples
        }
    
    def _save_results(self, results):
        """
        SonuÃ§larÄ± kaydet
        """
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        # JSON formatÄ±nda kaydet
        with open(results_dir / "evaluation_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        # CSV formatÄ±nda Ã¶zet kaydet
        summary_data = {
            "Metric": ["Accuracy", "F1 (Macro)", "F1 (Weighted)", "Precision", "Recall"],
            "Score": [
                results["metrics"]["accuracy"],
                results["metrics"]["f1_macro"],
                results["metrics"]["f1_weighted"],
                results["metrics"]["precision_macro"],
                results["metrics"]["recall_macro"]
            ]
        }
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(results_dir / "metrics_summary.csv", index=False)
        
        print(f"ğŸ’¾ SonuÃ§lar kaydedildi: {results_dir}")
    
    def test_custom_examples(self, examples=None):
        """
        Ã–zel Ã¶rnekler Ã¼zerinde test yap
        
        Args:
            examples (list): Test edilecek metin listesi
        """
        if examples is None:
            examples = [
                "Bu film gerÃ§ekten harika, Ã§ok beÄŸendim!",
                "Berbat bir deneyimdi, hiÃ§ memnun kalmadÄ±m.",
                "Fena deÄŸil, ortalama bir Ã¼rÃ¼n.",
                "MÃ¼kemmel hizmet, kesinlikle tavsiye ederim!",
                "Ã‡ok kÃ¶tÃ¼, paramÄ±n hakkÄ±nÄ± veremediler.",
                "Ä°dare eder, ne iyi ne kÃ¶tÃ¼.",
                "Hayal kÄ±rÄ±klÄ±ÄŸÄ± yaÅŸadÄ±m, beklentimi karÅŸÄ±lamadÄ±.",
                "SÃ¼per bir deneyim, tekrar geleceÄŸim!",
                "Vasat, daha iyisini gÃ¶rmÃ¼ÅŸtÃ¼m.",
                "Nefret ettim, asla tekrar almam."
            ]
        
        print("ğŸ§ª Ã–zel Ã¶rnekler test ediliyor...")
        print("=" * 50)
        
        results = []
        for text in examples:
            prediction = self.classifier(text)
            
            # En yÃ¼ksek skorlu tahmini al
            best_pred = max(prediction[0], key=lambda x: x['score'])
            
            result = {
                "text": text,
                "predicted_label": best_pred['label'],
                "confidence": best_pred['score'],
                "all_scores": {pred['label']: pred['score'] for pred in prediction[0]}
            }
            
            results.append(result)
            
            print(f"ğŸ“ Text: {text}")
            print(f"ğŸ·ï¸  Prediction: {best_pred['label']} (confidence: {best_pred['score']:.3f})")
            print(f"ğŸ“Š All scores: {', '.join([f'{pred['label']}: {pred['score']:.3f}' for pred in prediction[0]])}")
            print("-" * 50)
        
        return results

def main():
    """
    Ana deÄŸerlendirme fonksiyonu
    """
    print("ğŸ“Š Model Evaluation")
    print("=" * 30)
    
    # Evaluator'Ä± baÅŸlat
    evaluator = ModelEvaluator()
    
    if evaluator.model is None:
        print("âŒ Model bulunamadÄ±. Ã–nce eÄŸitim yapÄ±n: python src/model_training.py")
        return
    
    # KapsamlÄ± deÄŸerlendirme
    results = evaluator.evaluate_model()
    
    # Ã–zel Ã¶rnekler test et
    evaluator.test_custom_examples()
    
    print("\nâœ… DeÄŸerlendirme tamamlandÄ±!")
    print("ğŸ“ SonuÃ§lar 'results/' klasÃ¶rÃ¼nde kaydedildi.")

if __name__ == "__main__":
    main()
