"""
Turkish Sentiment Analysis Model Training Module
TÃ¼rkÃ§e sentiment analizi iÃ§in BERT model eÄŸitimi
"""

import os
import json
import numpy as np
import torch
from pathlib import Path
from datetime import datetime

from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    Trainer, 
    TrainingArguments,
    DataCollatorWithPadding,
    EarlyStoppingCallback
)

from datasets import DatasetDict, load_from_disk
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# WandB import (optional)
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸ WandB bulunamadÄ±. Experiment tracking olmayacak.")

class TurkishSentimentTrainer:
    """
    TÃ¼rkÃ§e sentiment analizi iÃ§in model eÄŸitici sÄ±nÄ±fÄ±
    """
    
    def __init__(self, 
                 model_name="dbmdz/bert-base-turkish-cased",
                 num_labels=3,
                 use_wandb=True):
        """
        Args:
            model_name (str): KullanÄ±lacak BERT model adÄ±
            num_labels (int): SÄ±nÄ±f sayÄ±sÄ± (positive, negative, neutral)
            use_wandb (bool): WandB kullanÄ±lsÄ±n mÄ±
        """
        self.model_name = model_name
        self.num_labels = num_labels
        self.use_wandb = use_wandb and WANDB_AVAILABLE
        
        # Model ve tokenizer'Ä± yÃ¼kle
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, 
            num_labels=num_labels
        )
        
        # Label mapping
        self.label_names = ["NEGATIVE", "NEUTRAL", "POSITIVE"]
        
        # GPU kontrolÃ¼
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ KullanÄ±lan device: {self.device}")
        
        if torch.cuda.is_available():
            print(f"ğŸš€ GPU: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        self.model.to(self.device)
        
    def compute_metrics(self, eval_pred):
        """
        Evaluasyon metrikleri hesapla
        
        Args:
            eval_pred: Trainer'dan gelen tahmin ve label'lar
            
        Returns:
            dict: Hesaplanan metrikler
        """
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        # Temel metrikler
        accuracy = accuracy_score(labels, predictions)
        f1_macro = f1_score(labels, predictions, average='macro')
        f1_weighted = f1_score(labels, predictions, average='weighted')
        precision = precision_score(labels, predictions, average='weighted')
        recall = recall_score(labels, predictions, average='weighted')
        
        # SÄ±nÄ±f bazÄ±nda F1 skorlarÄ±
        f1_per_class = f1_score(labels, predictions, average=None)
        
        metrics = {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'precision': precision,
            'recall': recall
        }
        
        # SÄ±nÄ±f bazÄ±nda metrikler ekle
        for i, class_name in enumerate(self.label_names):
            metrics[f'f1_{class_name.lower()}'] = f1_per_class[i]
        
        return metrics
    
    def plot_confusion_matrix(self, predictions, labels, save_path=None):
        """
        Confusion matrix gÃ¶rselleÅŸtir
        
        Args:
            predictions (array): Model tahminleri
            labels (array): GerÃ§ek label'lar
            save_path (str): Kaydedilecek dosya yolu
        """
        cm = confusion_matrix(labels, predictions)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.label_names,
                   yticklabels=self.label_names)
        plt.title('Confusion Matrix')
        plt.ylabel('GerÃ§ek Label')
        plt.xlabel('Tahmin Edilen Label')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Confusion matrix kaydedildi: {save_path}")
        
        plt.show()
        
    def load_dataset(self, dataset_path="data/processed/tokenized_dataset"):
        """
        Ä°ÅŸlenmiÅŸ dataset'i yÃ¼kle
        
        Args:
            dataset_path (str): Dataset dosya yolu
            
        Returns:
            DatasetDict: YÃ¼klenmiÅŸ dataset
        """
        try:
            dataset = load_from_disk(dataset_path)
            print(f"âœ… Dataset yÃ¼klendi: {dataset_path}")
            print(f"ğŸ“Š Train samples: {len(dataset['train'])}")
            print(f"ğŸ“Š Test samples: {len(dataset['test'])}")
            return dataset
        except Exception as e:
            print(f"âŒ Dataset yÃ¼klenemedi: {str(e)}")
            print("ğŸ’¡ Ã–nce data preprocessing Ã§alÄ±ÅŸtÄ±rÄ±n: python src/data_preprocessing.py")
            return None
    
    def train(self, 
              dataset=None,
              output_dir="./models/checkpoints",
              num_epochs=3,
              batch_size=16,
              learning_rate=2e-5,
              warmup_steps=500,
              weight_decay=0.01,
              eval_steps=500,
              save_steps=500,
              logging_steps=100):
        """
        Modeli eÄŸit
        
        Args:
            dataset (DatasetDict): EÄŸitim verisi
            output_dir (str): Model kaydetme dizini
            num_epochs (int): Epoch sayÄ±sÄ±
            batch_size (int): Batch boyutu
            learning_rate (float): Ã–ÄŸrenme oranÄ±
            warmup_steps (int): Warmup adÄ±m sayÄ±sÄ±
            weight_decay (float): Weight decay
            eval_steps (int): Evaluation adÄ±m sayÄ±sÄ±
            save_steps (int): Kaydetme adÄ±m sayÄ±sÄ±
            logging_steps (int): Log adÄ±m sayÄ±sÄ±
            
        Returns:
            Trainer: EÄŸitilmiÅŸ trainer objesi
        """
        print("ğŸš€ Model eÄŸitimi baÅŸlÄ±yor...")
        
        # Dataset yÃ¼kle
        if dataset is None:
            dataset = self.load_dataset()
            if dataset is None:
                return None
        
        train_dataset = dataset['train']
        eval_dataset = dataset['test']
        
        # WandB baÅŸlat
        if self.use_wandb:
            run_name = f"turkish-sentiment-{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            wandb.init(
                project="turkish-sentiment-analysis",
                name=run_name,
                config={
                    "model_name": self.model_name,
                    "num_epochs": num_epochs,
                    "batch_size": batch_size,
                    "learning_rate": learning_rate,
                    "warmup_steps": warmup_steps,
                    "weight_decay": weight_decay
                }
            )
        
        # Data collator
        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=4,  # Effective batch size = 4*4=16
            learning_rate=learning_rate,
            warmup_steps=warmup_steps,
            weight_decay=weight_decay,
            logging_dir='./logs',
            logging_steps=logging_steps,
            eval_strategy="steps",  # evaluation_strategy -> eval_strategy
            eval_steps=eval_steps,
            save_strategy="steps",
            save_steps=save_steps,
            load_best_model_at_end=True,
            metric_for_best_model="f1_weighted",
            greater_is_better=True,
            report_to="wandb" if self.use_wandb else None,
            dataloader_num_workers=0,  # Windows uyumluluÄŸu iÃ§in
            remove_unused_columns=False,
            push_to_hub=False,
            fp16=True,  # Mixed precision training for memory efficiency
            optim="adamw_torch",  # More memory efficient optimizer
            max_grad_norm=1.0  # Gradient clipping
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
        )
        
        # EÄŸitimi baÅŸlat
        print(f"ğŸ“š EÄŸitim baÅŸlÄ±yor...")
        print(f"   - Model: {self.model_name}")
        print(f"   - Train samples: {len(train_dataset)}")
        print(f"   - Eval samples: {len(eval_dataset)}")
        print(f"   - Epochs: {num_epochs}")
        print(f"   - Batch size: {batch_size}")
        print(f"   - Learning rate: {learning_rate}")
        
        trainer.train()
        
        # Final deÄŸerlendirme
        print("ğŸ“Š Final deÄŸerlendirme yapÄ±lÄ±yor...")
        eval_results = trainer.evaluate()
        
        print("âœ… EÄŸitim tamamlandÄ±!")
        print("ğŸ“ˆ Final sonuÃ§lar:")
        for key, value in eval_results.items():
            if key.startswith('eval_'):
                metric_name = key.replace('eval_', '')
                if isinstance(value, float):
                    print(f"   - {metric_name}: {value:.4f}")
        
        # Final modeli kaydet
        final_model_path = "./models/final"
        trainer.save_model(final_model_path)
        print(f"ğŸ’¾ Final model kaydedildi: {final_model_path}")
        
        # Confusion matrix oluÅŸtur ve kaydet
        predictions = trainer.predict(eval_dataset)
        y_pred = np.argmax(predictions.predictions, axis=1)
        y_true = predictions.label_ids
        
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        self.plot_confusion_matrix(
            y_pred, y_true, 
            save_path=results_dir / "confusion_matrix.png"
        )
        
        # SonuÃ§larÄ± JSON olarak kaydet
        final_metrics = {
            "model_name": self.model_name,
            "training_time": datetime.now().isoformat(),
            "final_metrics": eval_results,
            "training_config": {
                "num_epochs": num_epochs,
                "batch_size": batch_size,
                "learning_rate": learning_rate,
                "warmup_steps": warmup_steps,
                "weight_decay": weight_decay
            }
        }
        
        with open(results_dir / "training_results.json", "w", encoding="utf-8") as f:
            json.dump(final_metrics, f, ensure_ascii=False, indent=2)
        
        # WandB sonlandÄ±r
        if self.use_wandb:
            wandb.finish()
        
        return trainer
    
    def predict_text(self, text, model_path="./models/final"):
        """
        Tek bir metin iÃ§in tahmin yap
        
        Args:
            text (str): Tahmin yapÄ±lacak metin
            model_path (str): Model dosya yolu
            
        Returns:
            dict: Tahmin sonucu
        """
        # Model ve tokenizer yÃ¼kle
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # Metni tokenize et
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
        
        # Tahmin yap
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # SonuÃ§larÄ± formatla
        predicted_class = torch.argmax(predictions, dim=-1).item()
        confidence = predictions[0][predicted_class].item()
        
        result = {
            "text": text,
            "predicted_label": self.label_names[predicted_class],
            "confidence": confidence,
            "all_scores": {
                self.label_names[i]: predictions[0][i].item() 
                for i in range(len(self.label_names))
            }
        }
        
        return result

def main():
    """
    Ana eÄŸitim fonksiyonu
    """
    print("ğŸ‡¹ğŸ‡· Turkish Sentiment Analysis Training")
    print("=" * 50)
    
    # Trainer'Ä± baÅŸlat
    trainer = TurkishSentimentTrainer(
        model_name="dbmdz/bert-base-turkish-cased",
        num_labels=3,
        use_wandb=False  # Ä°lk test iÃ§in False
    )
    
    # EÄŸitimi baÅŸlat
    trained_trainer = trainer.train(
        num_epochs=2,  # GPU memory iÃ§in daha az epoch
        batch_size=4,  # 4.3GB GPU iÃ§in daha kÃ¼Ã§Ã¼k batch
        learning_rate=2e-5,
        warmup_steps=100,
        eval_steps=500,  # Daha az frequent evaluation
        save_steps=500,
        logging_steps=100
    )
    
    if trained_trainer:
        print("\nğŸ¯ Sonraki adÄ±mlar:")
        print("   1. python demos/gradio_demo.py - Demo uygulamasÄ±")
        print("   2. jupyter notebook notebooks/analysis.ipynb - DetaylÄ± analiz")
        print("   3. python src/evaluation.py - KapsamlÄ± deÄŸerlendirme")
        
        # Ã–rnek tahmin yap
        print("\nğŸ”® Ã–rnek tahminler:")
        test_texts = [
            "Bu film gerÃ§ekten harika, Ã§ok beÄŸendim!",
            "Berbat bir deneyimdi, hiÃ§ memnun kalmadÄ±m.",
            "Fena deÄŸil, ortalama bir Ã¼rÃ¼n."
        ]
        
        for text in test_texts:
            result = trainer.predict_text(text)
            print(f"   ğŸ“ '{text[:50]}...'")
            print(f"   ğŸ·ï¸  {result['predicted_label']} (gÃ¼ven: {result['confidence']:.3f})")
            print()

if __name__ == "__main__":
    main()
