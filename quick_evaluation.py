"""
Quick Model Evaluation Script
HÄ±zlÄ± model deÄŸerlendirmesi iÃ§in optimize edilmiÅŸ script
"""

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import random
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from datasets import load_from_disk
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix

class QuickEvaluator:
    """
    HÄ±zlÄ± model deÄŸerlendirme sÄ±nÄ±fÄ±
    """
    
    def __init__(self, model_path="./models/final", dataset_path="data/processed/tokenized_dataset"):
        self.model_path = model_path
        self.dataset_path = dataset_path
        self.label_names = ["NEGATIVE", "NEUTRAL", "POSITIVE"]
        
        # Model yÃ¼kle
        try:
            print(f"ğŸ”„ Model yÃ¼kleniyor: {model_path}")
            self.classifier = pipeline(
                "text-classification",
                model=model_path,
                device=0 if torch.cuda.is_available() else -1,
                return_all_scores=True
            )
            print("âœ… Model baÅŸarÄ±yla yÃ¼klendi")
            self.model_loaded = True
        except Exception as e:
            print(f"âŒ Model yÃ¼klenemedi: {e}")
            self.model_loaded = False
            return
        
        # Dataset yÃ¼kle
        try:
            self.dataset = load_from_disk(dataset_path)
            print(f"âœ… Dataset yÃ¼klendi: {len(self.dataset['test'])} test samples")
        except Exception as e:
            print(f"âŒ Dataset yÃ¼klenemedi: {e}")
            self.dataset = None
    
    def quick_evaluate(self, sample_size=1000):
        """
        HÄ±zlÄ± deÄŸerlendirme (subset ile)
        """
        if not self.model_loaded or self.dataset is None:
            print("âŒ Model veya dataset yÃ¼klenmemiÅŸ!")
            return None
        
        print(f"ğŸ“Š HÄ±zlÄ± deÄŸerlendirme baÅŸlÄ±yor ({sample_size} sample)...")
        
        # Test dataset'inden sample al
        test_dataset = self.dataset['test']
        if sample_size > len(test_dataset):
            sample_size = len(test_dataset)
            
        # Random sample al
        indices = random.sample(range(len(test_dataset)), sample_size)
        
        predictions = []
        true_labels = []
        confidences = []
        
        # Tokenizer yÃ¼kle
        tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        
        print("ğŸ”® Tahminler yapÄ±lÄ±yor...")
        for i, idx in enumerate(indices):
            if i % 100 == 0:
                print(f"Ä°ÅŸlenen: {i}/{sample_size}")
            
            example = test_dataset[idx]
            
            # Text'i decode et
            text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)
            
            try:
                # Tahmin yap
                result = self.classifier(text)
                best_pred = max(result[0], key=lambda x: x['score'])
                
                # Label formatÄ±nÄ± dÃ¼zelt
                if best_pred['label'].startswith('LABEL_'):
                    pred_label = int(best_pred['label'].split('_')[1])
                else:
                    label_map = {'NEGATIVE': 0, 'NEUTRAL': 1, 'POSITIVE': 2}
                    pred_label = label_map.get(best_pred['label'], 0)
                
                predictions.append(pred_label)
                true_labels.append(example['labels'])
                confidences.append(best_pred['score'])
                
            except Exception as e:
                print(f"Hata (index {idx}): {e}")
                continue
        
        # Metrikleri hesapla
        predictions = np.array(predictions)
        true_labels = np.array(true_labels)
        confidences = np.array(confidences)
        
        accuracy = accuracy_score(true_labels, predictions)
        f1_macro = f1_score(true_labels, predictions, average='macro')
        f1_weighted = f1_score(true_labels, predictions, average='weighted')
        precision = precision_score(true_labels, predictions, average='weighted')
        recall = recall_score(true_labels, predictions, average='weighted')
        
        print(f"\nğŸ“ˆ SonuÃ§lar ({sample_size} sample):")
        print(f"  ğŸ¯ Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)")
        print(f"  ğŸ“Š F1-Score (Macro): {f1_macro:.4f}")
        print(f"  ğŸ“Š F1-Score (Weighted): {f1_weighted:.4f}")
        print(f"  ğŸ” Precision: {precision:.4f}")
        print(f"  ğŸª Recall: {recall:.4f}")
        print(f"  ğŸ’ª Ortalama GÃ¼ven: {confidences.mean():.4f}")
        
        # Confusion matrix
        cm = confusion_matrix(true_labels, predictions)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.label_names,
                   yticklabels=self.label_names)
        plt.title(f'Confusion Matrix (n={sample_size})')
        plt.ylabel('GerÃ§ek Label')
        plt.xlabel('Tahmin')
        
        # SonuÃ§larÄ± kaydet
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        plt.savefig(results_dir / "quick_evaluation_cm.png", dpi=300, bbox_inches='tight')
        plt.show()
        
        # Classification report
        print("\nğŸ“‹ DetaylÄ± Classification Report:")
        print(classification_report(true_labels, predictions, target_names=self.label_names))
        
        # SonuÃ§larÄ± JSON olarak kaydet
        results = {
            "sample_size": sample_size,
            "accuracy": float(accuracy),
            "f1_macro": float(f1_macro),
            "f1_weighted": float(f1_weighted),
            "precision": float(precision),
            "recall": float(recall),
            "avg_confidence": float(confidences.mean()),
            "confusion_matrix": cm.tolist(),
            "classification_report": classification_report(true_labels, predictions, target_names=self.label_names, output_dict=True)
        }
        
        with open(results_dir / "quick_evaluation_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ SonuÃ§lar kaydedildi: {results_dir}")
        
        return results
    
    def test_custom_examples(self):
        """
        Ã–zel Ã¶rnekler test et
        """
        if not self.model_loaded:
            print("âŒ Model yÃ¼klenmemiÅŸ!")
            return
        
        examples = [
            "Bu film gerÃ§ekten harika, Ã§ok beÄŸendim!",
            "Berbat bir deneyimdi, hiÃ§ memnun kalmadÄ±m.",
            "Fena deÄŸil, ortalama bir Ã¼rÃ¼n.",
            "MÃ¼kemmel hizmet, kesinlikle tavsiye ederim!",
            "Ã‡ok kÃ¶tÃ¼, paramÄ±n hakkÄ±nÄ± veremediler.",
            "Ä°dare eder, ne iyi ne kÃ¶tÃ¼.",
            "Bu proje gerÃ§ekten baÅŸarÄ±lÄ± oldu!",
            "EÄŸitim sonuÃ§larÄ± harika Ã§Ä±ktÄ±!"
        ]
        
        print("ğŸ§ª Ã–zel Ã¶rnekler test ediliyor...")
        print("=" * 60)
        
        for i, text in enumerate(examples, 1):
            try:
                result = self.classifier(text)
                best_pred = max(result[0], key=lambda x: x['score'])
                
                # Emoji ekle
                emoji_map = {'NEGATIVE': 'ğŸ˜', 'NEUTRAL': 'ğŸ˜', 'POSITIVE': 'ğŸ˜Š'}
                if best_pred['label'].startswith('LABEL_'):
                    label_idx = int(best_pred['label'].split('_')[1])
                    predicted_label = self.label_names[label_idx]
                else:
                    predicted_label = best_pred['label']
                
                emoji = emoji_map.get(predicted_label, 'â“')
                
                print(f"{i:2d}. Text: {text}")
                print(f"    {emoji} {predicted_label} (gÃ¼ven: {best_pred['score']:.3f})")
                print()
                
            except Exception as e:
                print(f"{i:2d}. Text: {text}")
                print(f"    âŒ Hata: {e}")
                print()

def main():
    print("ğŸš€ Quick Model Evaluation")
    print("=" * 40)
    
    evaluator = QuickEvaluator()
    
    if evaluator.model_loaded and evaluator.dataset:
        # HÄ±zlÄ± deÄŸerlendirme
        results = evaluator.quick_evaluate(sample_size=2000)  # 2000 sample ile test
        
        # Ã–zel Ã¶rnekler
        evaluator.test_custom_examples()
        
        print("\nâœ… HÄ±zlÄ± deÄŸerlendirme tamamlandÄ±!")
        print("ğŸ“ DetaylÄ± sonuÃ§lar iÃ§in: results/quick_evaluation_results.json")
        
    else:
        print("âŒ Model veya dataset yÃ¼klenemedi!")

if __name__ == "__main__":
    main()
